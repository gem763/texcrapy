{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sejin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [i, work, on, natural, language, processing, a...\n",
       "1    [i, love, computer, science, and, i, code, in,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# routines:\n",
    "text = \"I work on natural language processing and I want to figure out how does gensim work\"\n",
    "text2 = \"I love computer science and I code in Python\"\n",
    "dat = pd.Series([text,text2])\n",
    "dat = dat.apply(lambda x: str(x).lower()) \n",
    "dat = dat.apply(lambda x: word_tokenize(x));dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 2),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 2)],\n",
       " [(0, 1), (5, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(dat)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in dat];corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'icecream', 'and', 'gensim']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "#Query:\n",
    "query_text = \"I love icecream and gensim\"\n",
    "query_text = query_text.lower()\n",
    "query_text = word_tokenize(query_text); query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.7071067811865475), (17, 0.7071067811865475)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_bow = dictionary.doc2bow(query_text)\n",
    "vec_tfidf = tfidf[vec_bow]; vec_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.       , 0.2581989, 0.2581989, 0.2581989, 0.2581989, 0.       ,\n",
       "        0.2581989, 0.2581989, 0.2581989, 0.2581989, 0.2581989, 0.2581989,\n",
       "        0.2581989, 0.5163978, 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       ], dtype=float32),\n",
       " array([0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
       "        0.       , 0.       , 0.4082483, 0.4082483, 0.4082483, 0.4082483,\n",
       "        0.4082483, 0.4082483], dtype=float32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import sparse2full\n",
    "import numpy as np\n",
    "\n",
    "[sparse2full(c, len(dictionary)) for c in corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 0.2581988897471611),\n",
       "  (2, 0.2581988897471611),\n",
       "  (3, 0.2581988897471611),\n",
       "  (4, 0.2581988897471611),\n",
       "  (6, 0.2581988897471611),\n",
       "  (7, 0.2581988897471611),\n",
       "  (8, 0.2581988897471611),\n",
       "  (9, 0.2581988897471611),\n",
       "  (10, 0.2581988897471611),\n",
       "  (11, 0.2581988897471611),\n",
       "  (12, 0.2581988897471611),\n",
       "  (13, 0.5163977794943222)],\n",
       " [(14, 0.4082482904638631),\n",
       "  (15, 0.4082482904638631),\n",
       "  (16, 0.4082482904638631),\n",
       "  (17, 0.4082482904638631),\n",
       "  (18, 0.4082482904638631),\n",
       "  (19, 0.4082482904638631)]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c for c in corpus_tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tokens_dict.json') as f:\n",
    "    tokens_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = list(tokens_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_train = fetch_20newsgroups(subset='train', categories=['sci.space','rec.autos'], data_home='~/local/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: prb@access.digex.com (Pat)\\nSubject: Re: Proton/Centaur?\\nOrganization: Express Access Online Communications USA\\nLines: 15\\nNNTP-Posting-Host: access.digex.net\\n\\n\\nWell thank you dennis for your as usual highly detailed and informative \\nposting.   \\n\\nThe question i have about the proton, is  could it be  handled at\\none of KSC's spare pads, without major  malfunction,  or could it be\\nhandled at kourou  or Vandenberg?   \\n\\nNow if it uses storables,  then  how long would it take for the russians\\nto equip something at cape york?\\n\\nIf  Proton were launched from a western site,  how would it compare to the\\nT4/centaur?   As i see it, it should lift  very close to the T4.\\n\\npat\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
